{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Initial setup of copula generative model based on parametric distributions for larger set of variables\n",
    "#\n",
    "# Author: Alex Braafladt\n",
    "#\n",
    "# Version: v1 Initial creation 3/11/2022\n",
    "#          v2 First draft with all components for nonparametric experiment for continuous case 3/14/2022\n",
    "#          v3 Draft with updates for hosting on GitHub 4/30/2022\n",
    "#\n",
    "# References:\n",
    "#   -https://openturns.github.io/openturns/latest/theory/probabilistic_modeling/copulas.html#copula\n",
    "#   -https://openturns.github.io/openturns/latest/auto_probabilistic_modeling/copulas/plot_composed_copula.html\n",
    "#   -https://en.wikipedia.org/wiki/Copula_(probability_theory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nonparametric Order Reduction Using Canonical Problem\n",
    "\n",
    "This notebook examines the ability to use the order reduction techniques to find a data-based basis of reduced dimensionality for the data generated using the marginal+copula joint distribution (generative model) that was set up to match the characteristics of the distributions expected in the outputs from agent-based simulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# probabilistic and statistical modeling\n",
    "import openturns as ot\n",
    "import openturns.viewer as viewer\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats as st\n",
    "\n",
    "# data and numerical functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# graphing and visualization functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# order reduction and data conditioners\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# design of experiments\n",
    "from doepy import build\n",
    "\n",
    "# os operations\n",
    "import os as os\n",
    "import sys\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "# custom functions\n",
    "import stats_functions as sf\n",
    "import copula_gen_data as cpgen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\alexb\\PycharmProjects\\distribution-surrogates\n",
      "Data save directory: C:\\Users\\alexb\\PycharmProjects\\distribution-surrogates\\output\\\\nonparOR20220502094726\n"
     ]
    }
   ],
   "source": [
    "# directory setup\n",
    "\n",
    "# get current working directory\n",
    "wrkdir = os.getcwd()\n",
    "print('Current working directory: '+wrkdir)\n",
    "# set up a data save directory for all future runs\n",
    "newoutputdir = wrkdir+'\\output'\n",
    "if not os.path.exists(newoutputdir):\n",
    "    os.makedirs(newoutputdir)\n",
    "# set up a new directory to store files for the current run - updates at each new full run of notebook\n",
    "curDatetime = dt.datetime.now()\n",
    "datasavedir = newoutputdir + r'\\\\' + 'nonparOR' + str(curDatetime.strftime('%Y%m%d%H%M%S'))\n",
    "if not os.path.exists(datasavedir):\n",
    "    os.makedirs(datasavedir)\n",
    "print('Data save directory: '+datasavedir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# set up emulator\n",
    "\n",
    "# defining objects here, to be moved to module later\n",
    "\n",
    "# Simulation class\n",
    "class Simulation:\n",
    "    \"\"\"\n",
    "    Object defining the joint distribution which represents an instance of a simulation\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    param_vals : np.array\n",
    "        input values based on simulation configuration (subset of all_param_vals)\n",
    "    all_param_vals : np.array\n",
    "        all values used to setup the specifics of the simulation\n",
    "    corr_rng_seed :\n",
    "        value to use as the seed for setting up the correlation matrix\n",
    "    distribution_types : list of strings\n",
    "        list of the specific types of distributions to be used\n",
    "    marginals : list of ot distributions\n",
    "        list of the ot distribution objects that define the marginal distributions\n",
    "    correlation_matrix : ot.CorrelationMatrix\n",
    "        correlation matrix between marginals\n",
    "    copula : ot.NormalCopula\n",
    "        Gaussian copula created based on the correlation matrix between marginals\n",
    "    joint_distribution : ot.ComposedDistribution\n",
    "        the main element of the simulation, combines the marginals through the copula\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_vals, corr_rng_seed):\n",
    "        self.input_vals = input_vals\n",
    "        self.param_vals = []\n",
    "        self.all_param_vals = []\n",
    "        self.corr_rng_seed = corr_rng_seed\n",
    "        self.distribution_types = []\n",
    "        self.marginals = []\n",
    "        self.correlation_matrix = None\n",
    "        self.copula = None\n",
    "        self.joint_distribution = None\n",
    "\n",
    "    def update_parameter_values(self):\n",
    "        \"\"\"set the values of the input parameters for the simulation\"\"\"\n",
    "        input_params = self.input_vals\n",
    "        param_list = [[input_params[4] * input_params[7], 3.0], [input_params[5] * 0.8, 10.0],\n",
    "                      [input_params[0],100.0], [-350.0,input_params[5]+10.*input_params[6]],\n",
    "                      [input_params[11]+0.3,0.3], [0.1*input_params[0], 1.2*input_params[1]],\n",
    "                      [input_params[-2], 3.0], [35000.0, input_params[1]], [input_params[-3]*4., 3.9],\n",
    "                      [-1.5, 1.0, 1.5, 1.0, 1.0 - input_params[4], input_params[4]],\n",
    "                      [input_params[2], 1.0, 5, 3.0, 0.5, 0.5],\n",
    "                      [55.0*input_params[4], 5.0, 85.0, 5.0, input_params[3], 1.0 - input_params[3]],\n",
    "                      [input_params[4]**2, 0.1, 0.9, 0.25, 0.5, 0.5], [input_params[4],1.0],\n",
    "                      [input_params[-5]*0.5,0.6], [35.0,input_params[5]], [3.0*input_params[6]-1.5,3.1],\n",
    "                      [input_params[6], 0.5, -1.0, 1.0], [5.0, 1.0*input_params[6], -1.0, 1.0],\n",
    "                      [2.0, input_params[7], -1.0, 1.0], [input_params[7], 5.0, -1.0, 1.0],\n",
    "                      [input_params[8], 2.0], [1.0*input_params[6], 2.0], [1.5, 3.0*input_params[6]],\n",
    "                      [1.5, input_params[9]], [20, input_params[10]], [30, input_params[11]],\n",
    "                      [input_params[12], 2.], [1, input_params[13]], [input_params[14]]]\n",
    "        self.param_vals = param_list\n",
    "\n",
    "    def update_distribution_types(self):\n",
    "        \"\"\"Create or replace the list of types of distributions to be included in marginals;\n",
    "        potentially an input in the future, for now, static\n",
    "        \"\"\"\n",
    "        dist_types = ['gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian', 'gaussian',\n",
    "                      'gaussian', 'gaussian', 'gaussian', 'gaussmix', 'gaussmix', 'gaussmix',\n",
    "                      'gaussmix', 'uniform', 'uniform', 'uniform', 'uniform', 'beta',\n",
    "                      'beta', 'beta', 'beta', 'gumbel', 'gumbel', 'gumbel',\n",
    "                      'gumbel', 'binomial', 'binomial', 'skellam', 'skellam', 'poisson']\n",
    "        self.distribution_types = dist_types\n",
    "\n",
    "    def update_marginals(self):\n",
    "        \"\"\"Use the param values to fill out the list of marginal distributions\"\"\"\n",
    "        # first, put specific inputs into distribution parameters list\n",
    "        self.update_parameter_values()\n",
    "        # then, list all the types of marginal distributions to create\n",
    "        self.update_distribution_types()\n",
    "        # then, for each dist_type : param_values pair, create the corresponding ot distribution\n",
    "        for (vals, name) in zip(self.param_vals, self.distribution_types):\n",
    "            if name == 'gaussian':\n",
    "                self.marginals.append(ot.Normal(vals[0], vals[1]))\n",
    "            elif name == 'gaussmix':\n",
    "                mix_margs = [ot.Normal(vals[0], vals[1]), ot.Normal(vals[2], vals[3])]\n",
    "                mix_weights = [vals[4], vals[5]]\n",
    "                self.marginals.append(ot.Mixture(mix_margs, mix_weights))\n",
    "            elif name == 'uniform':\n",
    "                self.marginals.append(ot.Uniform(vals[0], vals[1]))\n",
    "            elif name == 'beta':\n",
    "                self.marginals.append(ot.Beta(vals[0], vals[1], vals[2], vals[3]))\n",
    "            elif name == 'gumbel':\n",
    "                self.marginals.append(ot.Gumbel(vals[0], vals[1]))\n",
    "            elif name == 'binomial':\n",
    "                self.marginals.append(ot.Binomial(int(vals[0]), vals[1]))\n",
    "            elif name == 'skellam':\n",
    "                self.marginals.append(ot.Skellam(vals[0], vals[1]))\n",
    "            elif name == 'poisson':\n",
    "                self.marginals.append(ot.Poisson(vals[0]))\n",
    "            else:\n",
    "                print('Unexpected distribution name')\n",
    "                self.marginals.append(ot.Normal(0, 1))\n",
    "\n",
    "    def update_correlation_matrix(self):\n",
    "        \"\"\"Use the input random seed and shape of marginals to create the correlation matrix\"\"\"\n",
    "        # requires marginals to be created first\n",
    "        rng = np.random.default_rng(seed=self.corr_rng_seed)\n",
    "        num_distributions = len(self.marginals)\n",
    "        corr_samp = rng.uniform(low=0.0001, high=1.0, size=num_distributions)\n",
    "        norm_corr_samp = corr_samp / (sum(corr_samp))\n",
    "        local_corr_mat = norm_corr_samp * float(num_distributions)\n",
    "        rand_corr_mat = st.random_correlation.rvs(local_corr_mat, random_state=rng)\n",
    "        self.correlation_matrix = ot.CorrelationMatrix(num_distributions, rand_corr_mat.flatten())\n",
    "\n",
    "    def update_copula(self):\n",
    "        \"\"\"Use the correlation matrix to update the copula object for the simulation\"\"\"\n",
    "        self.update_correlation_matrix()\n",
    "        self.copula = ot.NormalCopula(self.correlation_matrix)\n",
    "\n",
    "    def update_joint_distribution(self):\n",
    "        \"\"\"Use the marginals and copula to update the joint distribution for the simulation\"\"\"\n",
    "        # create ot distributions for the marginals\n",
    "        self.update_marginals()\n",
    "        # create an ot copula to connect the ot distributions\n",
    "        self.update_copula()\n",
    "        # combine into a joint distribution\n",
    "        self.joint_distribution = ot.ComposedDistribution(self.marginals, self.copula)\n",
    "\n",
    "    def reset_simulation(self):\n",
    "        \"\"\"Remove setup of simulation to avoid appending duplicates\"\"\"\n",
    "        self.marginals = []\n",
    "\n",
    "    def get_joint_distribution_samples(self, n_samples=1000):\n",
    "        \"\"\"Get the specified number of random samples from the joint distribution that\n",
    "        defines the simulation\n",
    "        \"\"\"\n",
    "        self.reset_simulation()\n",
    "        self.update_joint_distribution()\n",
    "        samples = np.array(self.joint_distribution.getSample(n_samples))\n",
    "        return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "x_in = np.array([1000., 6500., -1.0, 0.2, 0.2, 45., 1.2, 3.0, 0.6, 2.2, 0.7, 0.6, 12., 8., 2.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "sim_ex = Simulation(x_in, 42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_samples = sim_ex.get_joint_distribution_samples(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Data class\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Object to hold different versions of data generated from multiple simulation calls\n",
    "    -conditions and converts to fill out required data types for Experiment\n",
    "    -path1: starts from samples and fills out other types\n",
    "    -path2: starts from ecdf_x and fills out other types\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scaled_ecdfs : tuple( ecdfy : np.array 1d, ecdfx : np.array 3d)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, samples=None, ecdf_vals=None, custom_bins=None, custom_scaler=None, discrete_flags=None):\n",
    "        self.samples = samples\n",
    "        self.custom_bins = custom_bins\n",
    "        self.custom_scaler = custom_scaler\n",
    "        self.scaler = None\n",
    "        self.scaled_samples = None\n",
    "        self.scaled_epdfs = None\n",
    "        self.scaled_ecdfs = ecdf_vals\n",
    "        self.discrete_flags = discrete_flags\n",
    "\n",
    "    def round_samples(self):\n",
    "        \"\"\"Round the samples which correspond to discrete distributions (assuming all discrete as int)\n",
    "        -requires samples set up\"\"\"\n",
    "        if self.discrete_flags is not None:\n",
    "            list_of_dist_data = np.split(self.samples, axis=2)\n",
    "            rounded_data_list = []\n",
    "            for (flag, data_slice) in zip(self.discrete_flags, list_of_dist_data):\n",
    "                if flag:\n",
    "                    data_slice = np.around(data_slice)\n",
    "                rounded_data_list.append(data_slice)\n",
    "            self.samples = np.stack(rounded_data_list, axis=2)\n",
    "\n",
    "    def scale_samples(self):\n",
    "        \"\"\"Scale samples based on custom scaler input or else sklearn StandardScaler - requires samples set up\"\"\"\n",
    "        if self.custom_scaler is None:\n",
    "            # use new sklearn standard scaler\n",
    "            scaler = StandardScaler()\n",
    "            # format to scale across full parametric dataset (scale to the full range)\n",
    "            samples_formatted_for_scaler = cpgen.format_data_for_global_scaler(self.samples)\n",
    "            scaler.fit(samples_formatted_for_scaler)\n",
    "            scaled_samples_in_scaler_format = scaler.transform(samples_formatted_for_scaler)\n",
    "            # reformat back to back shape\n",
    "            self.scaled_samples = cpgen.reshape_data_after_scaling(scaled_samples_in_scaler_format,\n",
    "                                                                   self.samples.shape[0])\n",
    "        else:\n",
    "            scaler = self.custom_scaler\n",
    "            samples_formatted_for_scaler = cpgen.format_data_for_global_scaler(self.samples)\n",
    "            scaled_samples_in_scaler_format = scaler.transform(samples_formatted_for_scaler)\n",
    "            self.scaled_samples = cpgen.reshape_data_after_scaling(scaled_samples_in_scaler_format,\n",
    "                                                                   self.samples.shape[0])\n",
    "\n",
    "    def get_scaled_ecdfs_from_samples(self):\n",
    "        \"\"\"Transform scaled samples into ecdf format - requires scaled samples set up\"\"\"\n",
    "        ecdf_y = sf.get_ecdf_y(self.scaled_samples[0,:,0])\n",
    "        ecdf_x = np.apply_along_axis(sf.get_ecdf_x, axis=1, arr=self.scaled_samples)\n",
    "        self.scaled_ecdfs = (ecdf_y, ecdf_x)\n",
    "\n",
    "    def get_scaled_epdfs_from_samples(self):\n",
    "        \"\"\"Transform scaled samples into epdf format - requires scaled samples set up\"\"\"\n",
    "        if self.custom_bins is None:\n",
    "            # determine bins from scaled_samples and hardcoded total number of bins\n",
    "            num_bins = 50\n",
    "            epdf_bins = np.apply_along_axis(sf.get_epdf_bins, arr=self.scaled_samples, bins=num_bins, axis=1)\n",
    "            epdf_probs = np.apply_along_axis(sf.get_epdf_probs, arr=self.scaled_samples, bins=num_bins, axis=1)\n",
    "        else:\n",
    "            # bins are given\n",
    "            bins_to_use = self.custom_bins\n",
    "            epdf_bins = np.zeros_like(self.scaled_samples)\n",
    "            epdf_probs = np.zeros_like(self.scaled_samples)\n",
    "            for i in range(epdf_bins.shape[0]):\n",
    "                for j in range(epdf_bins.shape[2]):\n",
    "                    bins = bins_to_use[i,:,j]\n",
    "                    epdf_bins = bins\n",
    "                    epdf_probs = sf.get_epdf_probs(arr=self.scaled_samples[i,:,j], bins=bins)\n",
    "        self.scaled_epdfs = (epdf_bins, epdf_probs)\n",
    "\n",
    "    def data_setup_from_samples(self):\n",
    "        \"\"\"Perform operations to flesh out data starting from a set of samples\"\"\"\n",
    "        # assuming samples already rounded\n",
    "        self.scale_samples()\n",
    "        self.get_scaled_ecdfs_from_samples()\n",
    "        self.get_scaled_epdfs_from_samples()\n",
    "\n",
    "    def data_setup_from_ecdfs(self):\n",
    "        \"\"\"Perform operations to flesh out data starting from a set of ecdfs\"\"\"\n",
    "        raw_ecdf_x_vals = self.scaled_ecdfs[1]\n",
    "        conditioned_ecdf_vals = np.apply_along_axis(sf.get_monotonic_ecdf_aprox, axis=1, arr=raw_ecdf_x_vals)\n",
    "        example_ecdf_y = sf.get_ecdf_y(conditioned_ecdf_vals[0,:,0])\n",
    "        undiscretized_samples = np.apply_along_axis(sf.sample_ecdf, arr=conditioned_ecdf_vals,\n",
    "                                                    axis=1, num_samples=conditioned_ecdf_vals.shape[1],\n",
    "                                                    ecdfy=example_ecdf_y)\n",
    "        # round the discrete samples\n",
    "        self.samples = undiscretized_samples\n",
    "        self.round_samples()\n",
    "        # update the ecdfx values for the discrete distributions\n",
    "        # todo - specify only the discrete ecdfs to get updated and leave the continuous ones\n",
    "        self.get_scaled_ecdfs_from_samples()\n",
    "        self.get_scaled_epdfs_from_samples()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "ex_samples1 = sim_ex.get_joint_distribution_samples(1000)\n",
    "ex_samples2 = sim_ex.get_joint_distribution_samples(1000)\n",
    "sampleset_ex = np.stack([ex_samples1, ex_samples2], axis=0)\n",
    "\n",
    "discrete_flags = []\n",
    "for i in range(25):\n",
    "    discrete_flags.append(False)\n",
    "for i in range(5):\n",
    "    discrete_flags.append(True)\n",
    "\n",
    "data_ex = Data(samples=sampleset_ex, discrete_flags=discrete_flags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_ex.data_setup_from_samples()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}